# docker-compose.yaml
name: telegram-bot

services:
  # Convex Backend Service
  convex-backend:
    image: ghcr.io/get-convex/convex-backend:c1a7ac393888d743e704de56cf569a154b4526d4
    stop_grace_period: 10s
    stop_signal: SIGINT
    deploy:
      resources:
        reservations:
          memory: ${NEXT_PUBLIC_CONVEX_BACKEND_RAM_RESERVATION:-1G}
    ports:
      - "${NEXT_PUBLIC_CONVEX_PORT:-3210}:3210"
      - "${NEXT_PUBLIC_CONVEX_SITE_PROXY_PORT:-3211}:3211"
    volumes:
      - convex_data:/convex/data
      - ./apps/docker-convex/admin-key:/convex/admin-key
    environment:
      - INSTANCE_NAME=${CONVEX_INSTANCE_NAME:-convex-telegram-bot}
      - INSTANCE_SECRET=${CONVEX_INSTANCE_SECRET:-0000000000000000000000000000000000000000000000000000000000000000}
      - CONVEX_RELEASE_VERSION_DEV=${CONVEX_RELEASE_VERSION_DEV:-}
      - ACTIONS_USER_TIMEOUT_SECS=${ACTIONS_USER_TIMEOUT_SECS:-600}
      - CONVEX_CLOUD_ORIGIN=${CONVEX_CLOUD_ORIGIN:-http://127.0.0.1:3210}
      - CONVEX_SITE_ORIGIN=${CONVEX_SITE_ORIGIN:-http://127.0.0.1:3211}
      - DATABASE_URL=${DATABASE_URL:-}
      - DISABLE_BEACON=${DISABLE_BEACON:-true}
      - REDACT_LOGS_TO_CLIENT=${REDACT_LOGS_TO_CLIENT:-false}
      - DO_NOT_REQUIRE_SSL=${DO_NOT_REQUIRE_SSL:-false}
      - POSTGRES_URL=${POSTGRES_URL:-}
      - MYSQL_URL=${MYSQL_URL:-}
      - RUST_LOG=${RUST_LOG:-info}
      - RUST_BACKTRACE=${RUST_BACKTRACE:-}
      - AWS_REGION=${AWS_REGION:-}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN:-}
      - S3_STORAGE_EXPORTS_BUCKET=${S3_STORAGE_EXPORTS_BUCKET:-}
      - S3_STORAGE_SNAPSHOT_IMPORTS_BUCKET=${S3_STORAGE_SNAPSHOT_IMPORTS_BUCKET:-}
      - S3_STORAGE_MODULES_BUCKET=${S3_STORAGE_MODULES_BUCKET:-}
      - S3_STORAGE_FILES_BUCKET=${S3_STORAGE_FILES_BUCKET:-}
      - S3_STORAGE_SEARCH_BUCKET=${S3_STORAGE_SEARCH_BUCKET:-}
      - S3_ENDPOINT_URL=${S3_ENDPOINT_URL:-}
      - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
      - CONVEX_URL=${CONVEX_URL:-http://localhost:3210}
      - CONVEX_SELF_HOSTED_URL=${CONVEX_SELF_HOSTED_URL:-http://localhost:3210}
      - CONVEX_DEPLOYMENT=${CONVEX_DEPLOYMENT:-self-hosted}
      - CONVEX_MAX_RAM_MB=${CONVEX_MAX_RAM_MB:-2048}
      - CONVEX_BOOTSTRAP_TIMEOUT_MS=${CONVEX_BOOTSTRAP_TIMEOUT_MS:-60000}
      - CONVEX_HEALTH_CHECK_TIMEOUT_MS=${CONVEX_HEALTH_CHECK_TIMEOUT_MS:-30000}
    healthcheck:
      test: curl -f http://localhost:3210/version
      interval: 5s
      start_period: 10s
      timeout: 5s
      retries: 5
    networks:
      - telegram-bot-network

  # Convex Dashboard Service
  convex-dashboard:
    image: ghcr.io/get-convex/convex-dashboard:c1a7ac393888d743e704de56cf569a154b4526d4
    stop_grace_period: 10s
    stop_signal: SIGINT
    deploy:
      resources:
        limits:
          memory: ${NEXT_PUBLIC_CONVEX_DASHBOARD_RAM:-256M}
        reservations:
          memory: ${NEXT_PUBLIC_CONVEX_DASHBOARD_RAM_RESERVATION:-128M}
    ports:
      - "${CONVEX_DASHBOARD_PORT:-6791}:6791"
    environment:
      - NEXT_PUBLIC_DEPLOYMENT_URL=${NEXT_PUBLIC_DEPLOYMENT_URL:-http://127.0.0.1:3210}
    depends_on:
      convex-backend:
        condition: service_healthy
    networks:
      - telegram-bot-network

  # Telegram bot written in Go
  # Only starts if TELEGRAM_TOKEN is provided
  telegram-bot:
      build:
        context: ./apps/golang-telegram-bot
        dockerfile: Dockerfile
      deploy:
        resources:
          limits:
            memory: ${NEXT_PUBLIC_TELEGRAM_BOT_RAM:-128M}
          reservations:
            memory: ${NEXT_PUBLIC_TELEGRAM_BOT_RAM_RESERVATION:-64M}
      environment:
        - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
        - CONVEX_URL=${CONVEX_URL:-http://convex-backend:3211}
        - CONVEX_INSTANCE_SECRET=${CONVEX_INSTANCE_SECRET:-0000000000000000000000000000000000000000000000000000000000000000}
        - VECTOR_CONVERT_LLM_URL=http://vector-convert-llm:7999
      depends_on:
        convex-backend:
          condition: service_healthy
        vector-convert-llm:
          condition: service_healthy
      restart: unless-stopped
      networks:
        - telegram-bot-network



  # Vector Convert LLM Service (Optimized & Stable)
  vector-convert-llm:
    build:
      context: ./apps/vector-convert-llm
      dockerfile: Dockerfile
    deploy:
      resources:
        limits:
          memory: ${NEXT_PUBLIC_VECTOR_CONVERT_LLM_RAM:-2G}
        reservations:
          memory: ${NEXT_PUBLIC_VECTOR_CONVERT_LLM_RAM_RESERVATION:-1G}
    ports:
      - "${VECTOR_CONVERT_PORT:-7999}:7999"
    environment:
      - PORT=7999
      - CONVEX_URL=http://convex-backend:3211
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - HF_HOME=/app/cache/huggingface
      - TOKENIZERS_PARALLELISM=false
      - OMP_NUM_THREADS=1
      - MKL_NUM_THREADS=1
      - OPENBLAS_NUM_THREADS=1
      - NUMEXPR_NUM_THREADS=1
      - VECLIB_MAXIMUM_THREADS=1
      - HF_HUB_OFFLINE=0
      - HF_HUB_DISABLE_TELEMETRY=1
    depends_on:
      convex-backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - telegram-bot-network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7999/health || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 5

  # Lightweight LLM service for document chat
  lightweight-llm:
      build:
        context: ./apps/lightweight-llm
        dockerfile: Dockerfile
      deploy:
        resources:
          limits:
            memory: ${NEXT_PUBLIC_LIGHTWEIGHT_LLM_RAM:-6G}
          reservations:
            memory: ${NEXT_PUBLIC_LIGHTWEIGHT_LLM_RAM_RESERVATION:-3G}
      ports:
      - "${LIGHTWEIGHT_LLM_PORT:-8082}:8082"
      environment:
        - PORT=8082
        - CONVEX_URL=${CONVEX_URL:-http://convex-backend:3211}
        - PYTHONUNBUFFERED=1
        - PYTHONDONTWRITEBYTECODE=1
        - MODELS_DIR=/models
        - HF_CACHE_DIR=/root/.cache/huggingface
        - HF_TOKEN=${HF_TOKEN}
        - DEFAULT_MODEL=${DEFAULT_MODEL:-meta-llama/Llama-3.2-1B-Instruct}
        - N_CTX=4096
        - N_THREADS=8
        - N_GPU_LAYERS=0
        - OMP_NUM_THREADS=8
        - MKL_NUM_THREADS=8
        - OPENBLAS_NUM_THREADS=8
        - NUMEXPR_NUM_THREADS=8
        - VECLIB_MAXIMUM_THREADS=8
        - INSTALL_LANGEXTRACT=${INSTALL_LANGEXTRACT:-false}
      volumes:
        - hf_models:/models
        - hf_cache:/root/.cache/huggingface
      depends_on:
        convex-backend:
          condition: service_healthy
      restart: on-failure:3
      networks:
        - telegram-bot-network
      healthcheck:
        test: ["CMD-SHELL", "curl -f http://localhost:8082/health || exit 1"]
        interval: 30s
        timeout: 15s
        start_period: 180s
        retries: 5

   # Next.js Web Dashboard Service
  web-dashboard:
    build:
      context: .
      dockerfile: ./apps/web/Dockerfile
      args:
        - NEXT_PUBLIC_CONVEX_URL=${NEXT_PUBLIC_CONVEX_URL:-http://localhost:3210}
        - NEXT_PUBLIC_CONVEX_WS_URL=${NEXT_PUBLIC_CONVEX_WS_URL:-ws://localhost:3210}
        - CONVEX_HTTP_URL=${CONVEX_HTTP_URL:-http://convex-backend:3211}
        - NEXT_PUBLIC_TELEGRAM_BOT_USERNAME=${TELEGRAM_BOT_USERNAME}
    deploy:
      resources:
        limits:
          memory: ${NEXT_PUBLIC_WEB_DASHBOARD_RAM:-1G}
        reservations:
          memory: ${NEXT_PUBLIC_WEB_DASHBOARD_RAM_RESERVATION:-512M}
    ports:
      - "${WEB_DASHBOARD_PORT:-3000}:3000"
    environment:
      - NEXT_PUBLIC_TOTAL_RAM_ALLOCATED=${NEXT_PUBLIC_TOTAL_RAM_ALLOCATED}
      - NEXT_PUBLIC_RAM_AVAILABLE=${NEXT_PUBLIC_RAM_AVAILABLE:-8G}
      - NEXT_PUBLIC_VECTOR_CONVERT_PORT=${VECTOR_CONVERT_PORT:-7999}
      - NEXT_PUBLIC_LIGHTWEIGHT_LLM_PORT=${LIGHTWEIGHT_LLM_PORT:-8082}
      - NEXT_PUBLIC_CONVEX_DASHBOARD_PORT=${CONVEX_DASHBOARD_PORT:-6791}
      - NEXT_PUBLIC_NEXT_PUBLIC_CONVEX_PORT=${NEXT_PUBLIC_CONVEX_SITE_PROXY_PORT:-3211}
      - NEXT_PUBLIC_CONVEX_URL=${NEXT_PUBLIC_CONVEX_URL:-http://localhost:3210}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_TOKEN}
      - NEXT_PUBLIC_TELEGRAM_BOT_USERNAME=${TELEGRAM_BOT_USERNAME}
      - CONVEX_URL=${CONVEX_URL:-http://convex-backend:3211}
      - CONVEX_HTTP_URL=${CONVEX_HTTP_URL:-http://convex-backend:3211}
      - NEXT_PUBLIC_CONVEX_WS_URL=${NEXT_PUBLIC_CONVEX_WS_URL:-ws://localhost:3210}
      - VECTOR_CONVERT_LLM_INTERNAL_URL=${VECTOR_CONVERT_LLM_INTERNAL_URL:-http://vector-convert-llm:7999}
      - VECTOR_CONVERT_LLM_URL=${VECTOR_CONVERT_LLM_URL:-http://vector-convert-llm:7999}
      - NEXT_PUBLIC_VECTOR_CONVERT_LLM_URL=${NEXT_PUBLIC_VECTOR_CONVERT_LLM_URL:-http://localhost:7999}
      - LIGHTWEIGHT_LLM_INTERNAL_URL=${LIGHTWEIGHT_LLM_INTERNAL_URL:-http://lightweight-llm:8082}
      - LIGHTWEIGHT_LLM_URL=${LIGHTWEIGHT_LLM_URL:-http://lightweight-llm:8082}
      - NEXT_PUBLIC_LIGHTWEIGHT_LLM_URL=${NEXT_PUBLIC_LIGHTWEIGHT_LLM_URL:-http://localhost:8082}
      - NODE_ENV=production
      - NEXT_PUBLIC_VECTOR_CONVERT_MODEL=${NEXT_PUBLIC_VECTOR_CONVERT_MODEL:-all-MiniLM-L6-v2}
      - NEXT_PUBLIC_VECTOR_CONVERT_MODEL_HUGGINGFACE_URL=${NEXT_PUBLIC_VECTOR_CONVERT_MODEL_HUGGINGFACE_URL:-https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}
      - NEXT_PUBLIC_LLM_MODEL=${NEXT_PUBLIC_LLM_MODEL:-Meta Llama 3.2}
      - NEXT_PUBLIC_LLM_MODEL_HUGGINGFACE_URL=${NEXT_PUBLIC_LLM_MODEL_HUGGINGFACE_URL:-https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}
    depends_on:
      convex-backend:
        condition: service_healthy
      lightweight-llm:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - telegram-bot-network

volumes:
  convex_data:
  hf_models:
  hf_cache:

networks:
  telegram-bot-network:
    driver: bridge

